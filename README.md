# ğŸ›¡ï¸ Neural Network Verification Toolbox

A dockerized service for formal verification of Neural Networks embedded in physical systems. This toolbox bridges the gap between machine learning models and mathematical optimization, providing safety and optimality guarantees.

---

## ğŸš€ Overview
This service allows users to verify Neural Network predictions against physical constraints or optimal solutions. It supports two primary verification modes:

* **Sub-Optimality Analysis (`check: distance`):** Measures the "Optimality Gap"â€”how far a NN prediction is from the mathematically certain "True Optimal" solution.
* **Safety Analysis (`check: constraint`):** Identifies "Worst-Case Violations"â€”searching for the specific input that forces the NN to break physical boundaries (e.g., thermal limits, power balance).



---

## ğŸ› ï¸ Project Structure
```text
.
â”œâ”€â”€ main.py                 # Service Entrypoint & Agnostic Router
â”œâ”€â”€ config.yaml             # Single Source of Truth (Problem Definition)
â”œâ”€â”€ usecase/                # Problem Class Logic (optimization, control, forecast)
â”œâ”€â”€ verify/                 # Core Verification Engines (MILP, CROWN)
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ reports.ipynb       # Generated timestamped .ipynb files
â”‚   â””â”€â”€ utils/              # Report Generation Logic (nbformat)
â””â”€â”€ Dockerfile              # Containerization for reproducible environments (to be implemented)


## ğŸš€ Getting Started

To get started, please follow the instructions below.

```text

```

## âš™ï¸ Configuration Reference

The behavior of the toolbox is governed by the `model_meta` section in your `config.yaml`. This section acts as the "brain" for the agnostic router and the reporting engine.

### ğŸ“ Metadata Breakdown

| Key | Example | Description |
| :--- | :--- | :--- |
| `name` | `"lp_proxy"` | A unique identifier for the model. This string is used in the report headers and file naming. |
| `pclass` | `"optimization"` | **Routing Class:** Determines the high-level logic folder. Currently supports `optimization`, with hooks for `control` or `forecast` in development. |
| `ptype` | `"lp"` | **Problem Type:** Specifies the mathematical structure (e.g., Linear Programming, Mixed-Integer, or Quadratic). This directs the system to the correct solver interface. |
| `check` | `"constraint"` | **Verification Mode:** <br>â€¢ `constraint`: Searches for safety violations (worst-case $Ax > b$). <br>â€¢ `distance`: Measures the optimality gap against the ground-truth. |
| `report` | `"yes"` | **Reporting Toggle:** Accepts `yes`/`no` or `true`/`false`. If enabled, a Jupyter Notebook is automatically generated in the `output/` directory. |
| `architecture` | `"feedforward"`| **Model Type:** Informs the `NNLoader` how to parse the weights (e.g., `feedforward` for MLPs or `cnn` for convolutional nets). |
| `engine` | `"milp"`| **Verification Engine:** Selects the solver. `milp` is complete (provides exact worst-case results) but slow. `crown` is incomplete (provides a fast formal upper bound) and may include a relaxation gap. |
| `activation` | `"relu"` | **Activation Function:** Specifies the non-linearity. This is vital for MILP-based verification, which requires specific encodings for `relu` vs `sigmoid`. |

---

### ğŸš€ Logic Flow
Based on these settings, the `main.py` entry point follows this decision logic:

1. **Routing:** Uses `pclass` and `ptype` to find the correct `runner` in the `USECASE_ROUTER`.
2. **Task Definition:** Uses `check` to determine whether to build a **Feasibility** MILP (violation search) or a **Regret** MILP (distance search).
3. **Execution:** Solves the verification problem.
4. **Reporting:** If `report` is `yes`, it triggers the `report_gen` utility using the mode-specific template.
